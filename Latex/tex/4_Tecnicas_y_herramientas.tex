\capitulo{4}{Técnicas y herramientas}

Esta parte de la memoria tiene como objetivo presentar las técnicas metodológicas y las herramientas de desarrollo que se han utilizado para llevar a cabo el proyecto. Si se han estudiado diferentes alternativas de metodologías, herramientas, bibliotecas se puede hacer un resumen de los aspectos más destacados de cada alternativa, incluyendo comparativas entre las distintas opciones y una justificación de las elecciones realizadas. 
No se pretende que este apartado se convierta en un capítulo de un libro dedicado a cada una de las alternativas, sino comentar los aspectos más destacados de cada opción, con un repaso somero a los fundamentos esenciales y referencias bibliográficas para que el lector pueda ampliar su conocimiento sobre el tema.

\section{Tecnología de scrapping: Scrappy}


En el ámbito del web scraping existen múltiples herramientas y bibliotecas que facilitan la extracción de datos de sitios web para su posterior análisis o almacenamiento. La Tabla \ref{tabla:tecnologiasComparadas} compara algunas de las tecnologías de web scraping más conocidas, evaluándolas en base a varios criterios como el rendimiento, facilidad de uso, y flexibilidad.

Para el proyecto actual, que tiene como objetivo extraer información relevante sobre la compraventa de inmuebles en pisos.com, se ha seleccionado Scrapy como la tecnología de scraping a utilizar. A continuación, se describen las razones de esta elección:

\begin{description}
	\item[Rendimiento] Scrapy es conocido por su alta velocidad y eficiencia, permitiendo la extracción de grandes cantidades de datos en un tiempo reducido. Esto es especialmente útil para nuestro proyecto, donde la actualización frecuente de los anuncios y las ofertas es una constante.
	\item[Facilidad de Uso] Aunque Scrapy tiene una curva de aprendizaje inicial más pronunciada en comparación con, por ejemplo, Beautiful Soup, ofrece una gran cantidad de funcionalidades "out-of-the-box" que aceleran el proceso de desarrollo una vez se comprende su funcionamiento básico.
    \item[Flexibilidad:] Scrapy es altamente configurable y extensible, lo cual permite adaptar el scraper a necesidades específicas. Esto resulta particularmente útil cuando se trata de sitios web con estructuras más complejas o cuando se requieren funcionalidades avanzadas como el manejo de sesiones, cookies o cabeceras HTTP.
    \item[Madurez y Comunidad:] Scrapy es una tecnología madura con una comunidad activa, lo que asegura un buen soporte y una amplia disponibilidad de documentación y recursos de aprendizaje.
    \item[Preferencia del autor para aprender la herramienta:] Algunas de las herramientas como Beautiful Soup y Selenium ya habían sido usadas por el autor, por lo tanto uno de los motivos para decidir por Scrapy fue el hecho de aprender una nueva herramienta ampliamente usada en la comunidad.
\end{description}

Por todas estas razones, Scrapy se presenta como la opción más robusta y versátil para el web scraping de \url{www.pisos.com}, proporcionando las herramientas necesarias para llevar a cabo un proyecto exitoso.



\tablaSmall{Comparación de Tecnologías de Web Scraping}{l c c c c}{tecnologiasComparadas}
{ \multicolumn{1}{l}{Tecnologías} & Rendimiento & Facilidad de Uso & Flexibilidad & Proyecto \\}{
Scrapy & \textbf{XX} & X & \textbf{XX} & \textbf{XX}\\
Beautiful Soup & X & \textbf{XX} & X & \\
Selenium & X & X & X & \\
Requests & X & \textbf{XX} & X & \\
Mechanize & X & X & X & \\
}

\section{Base de datos primaria: MongoDB}

Los datos recopilados a través de scrapping deben almacenarse de forma permanente para su posterior uso. Esto requiere inevitablemente usar una base de datos.

Una de las primeras decisiones fue usar una base de datos no relacional, por la flexibilidad que podía aportar al tratarse de datos de scrapping. Es frecuente que estos datos no tengan una estructura muy definida o que varíen considerablemente en formato, campos disponibles o tipos de datos. En tales casos, las bases de datos relacionales pueden resultar restrictivas, ya que exigen un esquema fijo y predefinido que todos los registros deben seguir. Por el contrario, MongoDB permite una estructura más flexible, lo que hace que sea más adecuada para manejar datos heterogéneos.

Dentro de las bases de datos NoSQL, otra decisión rápida fue la de elegir una que fuera Open Source y con licencia gratuita. Esto no solo ayuda a mantener bajos los costos del proyecto, sino que también abre la puerta a una amplia comunidad de desarrolladores y una gran cantidad de recursos y documentación en línea. De las bases de datos que cumplían con estos requisitos, MongoDB destacó por varias razones:

\begin{itemize}
\item \textbf{Prevalencia en la Industria}: MongoDB es una de las bases de datos NoSQL más populares y ampliamente utilizadas. Esto no solo la hace una tecnología atractiva para aprender desde una perspectiva de desarrollo profesional, sino que también asegura una amplia gama de soporte comunitario y empresarial.

\item \textbf{Experiencia Previa}: Contar con experiencia previa en MongoDB reduce significativamente la curva de aprendizaje, permitiendo un desarrollo más rápido y eficiente del proyecto.

\item \textbf{Escalabilidad}: MongoDB ofrece una excelente escalabilidad horizontal, lo que permite manejar grandes volúmenes de datos y tráfico sin degradar el rendimiento. Esto es especialmente útil para proyectos de scrapping que pueden empezar pequeños pero crecer rápidamente.

\item \textbf{Consultas Flexibles}: MongoDB ofrece un sistema de consultas flexible y potente que permite realizar búsquedas complejas, algo que es especialmente útil cuando se trata de analizar y utilizar los datos recopilados.

\item \textbf{Integración con otras Tecnologías}: MongoDB se integra fácilmente con numerosas plataformas y lenguajes de programación, lo que la convierte en una opción versátil para cualquier stack tecnológico. La integración son scrapy, la libreria de python para web scrapping utilizada fue trivial gracias a la libreria pymongo, la libreria de mongodb para python.
\end{itemize}

Por todas estas razones, MongoDB se convirtió en la opción más atractiva para este proyecto, ofreciendo la combinación ideal de flexibilidad, escalabilidad y soporte comunitario.

\section{Base de datos secundaria: SQLite}

Los datos originados de la base de datos primaria y transformados mediante el proceso ETL (Extracción, Transformación y Carga) son nuevamente almacenados en una base de datos. Para este paso del flujo de datos, se optó por una base de datos relacional como SQLite por diversas razones:

\begin{itemize}
\item \textbf{Búsquedas y Ordenaciones Rápidas}: Las bases de datos relacionales son particularmente eficientes en la realización de consultas que involucran ordenamientos y joins, lo que resulta útil para las interfaces web donde se necesita acceder a los datos de forma rápida y eficiente.

\item \textbf{Estructura Clara}: Este tipo de base de datos proporciona un esquema bien definido, lo cual facilita el análisis de datos y las operaciones de machine learning, al no requerir transformaciones adicionales para su uso.

\item \textbf{Economía de Recursos}: Dado que el volumen de datos a manejar es relativamente pequeño (inferior a 500 MB cada 2-3 meses de recolección), una base de datos ligera y eficiente como SQLite es más que suficiente para satisfacer las necesidades del proyecto.

\item \textbf{Integración con Python}: SQLite permite una integración directa con Python, lo que simplifica significativamente el flujo de trabajo y evita la necesidad de implementar y mantener un servidor de base de datos separado.

\item \textbf{Facilidad de Migración}: En caso de que el proyecto escale y requiera una solución más robusta, la migración a una base de datos relacional más potente, como PostgreSQL o MySQL, sería un proceso relativamente sencillo. Esto es debido a que el esquema y las consultas SQL podrían reutilizarse con pocos ajustes.
\end{itemize}

Por lo tanto, SQLite se convierte en una excelente opción para este escenario específico. Ofrece la ventaja de ser ligero y eficiente en el uso de recursos, mientras proporciona todas las funcionalidades necesarias para realizar análisis de datos y machine learning de forma eficaz. Además, su fácil integración con Python y la flexibilidad para una futura migración hacen de SQLite una elección pragmática y efectiva para este proyecto.

\section{Herramientas de ETL: Pandas}

Para el proceso de Extracción, Transformación y Carga (ETL) de los datos, se decidió utilizar Python con la ayuda de la librería Pandas. A continuación, se describen las razones que llevaron a esta elección:

\begin{itemize}
\item \textbf{Volumen de Datos}: Uno de los principales factores fue el tamaño moderado del conjunto de datos con el que se está trabajando. Con aproximadamente 500 MB de datos recopilados cada 2-3 meses, el volumen no justifica el uso de una herramienta diseñada para el procesamiento de datos masivos, como Spark.

\item \textbf{Eficiencia y Velocidad}: Con Pandas y Python, la transformación de toda la base de datos se puede realizar en cuestión de segundos. Esto significa que no hay una necesidad inmediata de una infraestructura más compleja y potente. Usar Spark en este contexto sería una forma de sobreingeniería que añadiría complejidad innecesaria al proyecto.

\item \textbf{Recursos de Máquina}: Spark exige una asignación de recursos considerablemente mayor que Pandas, especialmente si se configura en un modo distribuido. Estos recursos podrían ser más eficientemente dedicados a otras tecnologías o aspectos del proyecto.

\item \textbf{Facilidad y flexibilidad}: Pandas ofrece una API intuitiva y fácil de usar, lo que acelera el desarrollo y facilita el mantenimiento. Además, la comunidad de Pandas es amplia, con una gran cantidad de documentación y tutoriales disponibles. Pandas es extremadamente flexible y permite una amplia gama de operaciones de manipulación de datos, desde simples filtrados y ordenaciones hasta operaciones de agrupación y pivoteo más complejas.

\item \textbf{Costo y Licencia}: Al ser una biblioteca de código abierto, Pandas no incurre en costos adicionales, lo cual es beneficioso desde el punto de vista económico del proyecto.
\end{itemize}

Por todas estas razones, se optó por utilizar Python con Pandas para las operaciones de ETL en este proyecto. Esta elección se alinea con los requisitos y limitaciones del proyecto, ofreciendo una solución que es tanto eficiente como efectiva, sin incurrir en la complejidad y los costos adicionales que implicaría la implementación de una herramienta como Spark.

\section{Orquestador: Apache airflow}

Explicar por qué se decidió el uso de airflow como orquestador

\section{Machine learning: scikit learn}

Las secciones se incluyen con el comando section.

\section{Aplicación web: Frontend - React}

Las secciones se incluyen con el comando section.

\section{Aplicación web: Backend - Node.js, express}

Las secciones se incluyen con el comando section.

\section{Despliegue - Oracle cloud}

Las secciones se incluyen con el comando section.
